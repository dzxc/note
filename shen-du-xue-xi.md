# 深度学习

## 前言
知识准备
    Python
    微积分
    线性代数
公式一览
    逻辑回归 
    $$\hat y^{(i)}=\sigma(w^TX^{(i)}+b)$$


    激活函数()
$$\sigma(z^{(i)})= \frac{1}{1+e^{-z^{(i)}}}$$

    损失函数(loss function)
$$L(\hat y^{(i)}, y^{(i)})= -(y^{(i)}log(\hat y^{(i)})) + (1-y^{(i)})log(1-\hat y^{(i)})$$
    
    
    成本函数(cost function)
$$J(w,b) = \frac{1}{m}\sum_{i=1}^{m}L(\hat y^{(i)},y^{(i)})=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}log(\hat y^{(i)})+(1-y^{(i)})log(1-\hat y^{(i)})]$$


    梯度下降函数
        
    
    
概念

### 数据输入与矩阵

矩阵：

非结构数据类
图片:
图片由一个个像素点构成一个矩形，图片有长和宽，如64*64，每个像素由r（red）g(green)b(blue)三种颜色组成，

音频：

结构数据类


## 神经网络入门

### 什么是神经网络

神经元与神经网络
人工神经网络

神经网络越简单智商就越低，神经网络越复杂智商就越高


训练深度神经网络就是深度学习

### 神经元与神经网络

### 人工神经网络

### 神经网络如何预测
#### 逻辑回归

#### 激活函数引入
通过逻辑回归计算得出的结果不能直接作为预测结果，需要激活函数激活
#### 损失函数引入
 计算预测结果与真实结果的差，就是损失函数
 
### 神经网络学习流程
输入数据
前向传播
反向传播
梯度下降


## 前向传播
### 逻辑回归
### 激活函数
### 损失函数
### 成本函数

## 反向传播

## 相关函数导数计算

## 梯度下降

## 激活函数
### sigmoid
### tanh
### relu
### leaky-relu




## 神经网络调优
### 神经网络缺陷
### 过拟合与欠拟合
### 梯度消失与梯度爆炸



## 神经网络实战